{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from imageio import imread, imwrite\n",
    "\n",
    "import os, copy, time\n",
    "\n",
    "from horsetools import list_files\n",
    "from fcn import VGGNet, FCNs\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "    \n",
    "class SegmentationDataset(Dataset):\n",
    "    IMG_EXTS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif')\n",
    "    \n",
    "    def _get_files(self, folder):\n",
    "        if os.path.isdir(folder):\n",
    "            return sorted(list_files(folder, valid_exts=self.IMG_EXTS))\n",
    "        else:\n",
    "            raise(RuntimeError('No folder named \"{}\" found.'.format(folder)))\n",
    "    \n",
    "    def __init__(self, root, labels, image_transforms=None, mask_transforms=None):\n",
    "        self.imgs = self._get_files(os.path.join(root, 'images'))\n",
    "        self.masks = self._get_files(os.path.join(root, 'masks'))\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.image_transforms = image_transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = default_loader(self.imgs[index])\n",
    "        mask = default_loader(self.masks[index])\n",
    "        if self.image_transforms is not None:\n",
    "            img = self.image_transforms(img)\n",
    "        if self.mask_transforms is not None:\n",
    "            mask = self.mask_transforms(mask)\n",
    "            \n",
    "        mask = mask.long()\n",
    "            \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# find means and stds of dataset\n",
    "imgs_list = list_files('stroma_epithelia/images')\n",
    "np_means = np.zeros((len(imgs_list), 3))\n",
    "np_stds = np.zeros_like(np_means)\n",
    "for i, img_name in enumerate(imgs_list):\n",
    "    img = imread(img_name)\n",
    "    np_means[i] = np.mean(img, axis=(0, 1))\n",
    "    np_stds[i] = np.std(img, axis=(0, 1))\n",
    "    \n",
    "channel_means = np.mean(np_means, axis=0)\n",
    "channel_stds = np.std(np_stds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Means: {}'.format(channel_means))\n",
    "print('Stds: {}'.format(channel_stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_root = 'stroma_epithelia'\n",
    "crop_size = 224\n",
    "labels = (0, 1, 2)\n",
    "dataset_phases = ['train']\n",
    "\n",
    "n_labels = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LabelToOnehot(object):\n",
    "    def __init__ (self, labels):\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        \n",
    "        if len(img.shape) > 2:\n",
    "            img = img[:, :, 0]\n",
    "            \n",
    "        onehot = np.zeros((img.shape[0], img.shape[1], len(self.labels)))\n",
    "        for i, l in enumerate(self.labels):\n",
    "            onehot[:, :, i] = img == l\n",
    "            \n",
    "        return onehot\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': {\n",
    "        'imgs': transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(channel_means, channel_stds)\n",
    "        ]),\n",
    "        'masks': transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            LabelToOnehot(labels),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    }\n",
    "}\n",
    "\n",
    "image_datasets = {x: SegmentationDataset(data_root, labels, \n",
    "                                         image_transforms=data_transforms[x]['imgs'],\n",
    "                                         mask_transforms=data_transforms[x]['masks'])\n",
    "                  for x in dataset_phases}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=4, \n",
    "                             shuffle=True, num_workers=4)\n",
    "               for x in dataset_phases}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in dataset_phases}\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test onehot \n",
    "mask_name = 'stroma_epithelia/masks/0.1_110_2_1.png'\n",
    "mask = imread(mask_name)\n",
    "LO = LabelToOnehot(labels)\n",
    "mask_oh = LO(mask)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(mask)\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Visualize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def imshow(inp, means=None, stds=None, title=None):\n",
    "    # convert tensor back to image range [0, 1]\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    if means is not None and stds is not None:\n",
    "        inp = np.array(stds) * inp + np.array(means)\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "print('Image shape: {}'.format(inputs.shape))\n",
    "print('Masks shape: {}'.format(masks.shape))\n",
    "imshow(torchvision.utils.make_grid(inputs), means=channel_means, stds=channel_stds)\n",
    "imshow(torchvision.utils.make_grid(masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, \n",
    "                phases, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # display epoch\n",
    "        print('Epoch {} / {}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            # reset loss for current phase and epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # track history only during training phase\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def visualize_model(model, dataloaders, labels, num_images=6):\n",
    "    # save model training state to reset once finished\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # without applying gradients\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # evaluate inputs\n",
    "            outputs = model(inputs)\n",
    "            # find argmax of class predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # for all images in inputs\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images // 2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(labels[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "                \n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "                \n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learn_rate = 0.001\n",
    "momentum = 0.9\n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n",
    "fcn_model = FCNs(pretrained_net=vgg_model, n_class=n_labels)\n",
    "\n",
    "fcn_model = fcn_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(fcn_model.parameters(), lr=learn_rate, momentum=momentum)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fcn_model = train_model(fcn_model, dataloaders, criterion, optimizer_ft, \n",
    "                        scheduler, dataset_phases, num_epochs=25)\n",
    "visualize_model(fcn_model, dataloaders, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
