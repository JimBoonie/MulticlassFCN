{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread, imwrite\n",
    "\n",
    "from fcn import VGGNet, FCNs\n",
    "from dataset import SegmentationDataset, LabelToOnehot, imshow\n",
    "from train import train_model, visualize_model\n",
    "\n",
    "from util import list_files\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find normalization constants for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading images into the model to train, we wish to define a normalization procedure that forces the input data to have roughly zero mean and unit variance. This ensures that all possible inputs exist within the same input space and at the same scale, facilitating the network's learning capability. \n",
    "\n",
    "To find these normalization constants, we loop through the training data and compute the mean and std of each image. This can take a while, so feel free to skip to the next cell, which defines the means and stds using precomputed literals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find means and stds of dataset\n",
    "imgs_list = list_files('stroma_epithelia/images')\n",
    "np_means = np.zeros((len(imgs_list), 3))\n",
    "np_stds = np.zeros_like(np_means)\n",
    "for i, img_name in enumerate(imgs_list):\n",
    "    img = imread(img_name)\n",
    "    np_means[i] = np.mean(img, axis=(0, 1))\n",
    "    np_stds[i] = np.std(img, axis=(0, 1))\n",
    "    \n",
    "channel_means = np.mean(np_means, axis=0)\n",
    "channel_stds = np.std(np_stds, axis=0)\n",
    "\n",
    "print('Means: {}'.format(channel_means))\n",
    "print('Stds: {}'.format(channel_stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use precomputed means and stds\n",
    "channel_means = (164.60564156, 150.75147313, 178.36823845)\n",
    "channel_stds = (9.46653235, 16.23701039, 9.48000306)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of the input images are not consistent. As a quick fix, we crop the [224, 224] center square out of every image. These are then converted to PyTorch tensors and then normalized using the constants earlier.\n",
    "\n",
    "The label masks must be cropped exactly as their corresponding images. In addition, they must be converted from labels into their one-hot encoding. This ensures that each label's representation in the network is statistically independent of the others. This would not be the case if we tried to represent each label as the values '0', '1', or '2', since '1' is halfway between '0' and '2' for instance.\n",
    "\n",
    "Since we are dealing with histology images, normally we would want to make sure that every image is taken at or rescaled to be at a consistent magnification. However, we know these images are the same scale, so no resizing is performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root = 'stroma_epithelia'\n",
    "crop_size = 224\n",
    "labels = (0, 1, 2)\n",
    "dataset_phases = ['train']\n",
    "\n",
    "n_labels = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': {\n",
    "        'imgs': transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(channel_means, channel_stds)\n",
    "        ]),\n",
    "        'masks': transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            LabelToOnehot(labels),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    }\n",
    "}\n",
    "\n",
    "image_datasets = {x: SegmentationDataset(data_root, labels, \n",
    "                                         image_transforms=data_transforms[x]['imgs'],\n",
    "                                         mask_transforms=data_transforms[x]['masks'])\n",
    "                  for x in dataset_phases}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=4, \n",
    "                             shuffle=True, num_workers=4)\n",
    "               for x in dataset_phases}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in dataset_phases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare label and mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we manually load one mask, count its labels, and display its one-hot encoded version. The encoded version should have the same structure as the label version, indicating that only individual pixel values have been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_name = 'stroma_epithelia/masks/0.1_110_2_1.png'\n",
    "mask = imread(mask_name)\n",
    "LO = LabelToOnehot(labels)\n",
    "mask_oh = LO(mask)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(mask)\n",
    "plt.title('Label mask')\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_oh)\n",
    "plt.title('One-hot mask \\n(R=0, G=1, B=2)')\n",
    "\n",
    "labels, label_counts = np.unique(mask, return_counts=True)\n",
    "for l, lc in zip(labels, label_counts):\n",
    "    print('Label {}: {} pixels'.format(l, lc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we test the dataloader by loading batches of images and masks and displaying them in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "print('Image shape: {}'.format(inputs.shape))\n",
    "print('Masks shape: {}'.format(masks.shape))\n",
    "\n",
    "imshow(torchvision.utils.make_grid(inputs), means=channel_means, stds=channel_stds)\n",
    "imshow(torchvision.utils.make_grid(masks), means=(0, 0, 0), stds=(255, 255, 255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the model and its training parameters. We employ the FCN variation of the pretrained VGGNet, which should vastly improve training convergence and performance.\n",
    "\n",
    "For a loss, we apply the binary cross entropy with logits loss, since we wish to measure the error of predictions between 0 and 1 across several logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.001\n",
    "momentum = 0.9\n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n",
    "fcn_model = FCNs(pretrained_net=vgg_model, n_class=n_labels)\n",
    "\n",
    "fcn_model = fcn_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(fcn_model.parameters(), lr=learn_rate, momentum=momentum)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcn_model = train_model(fcn_model, dataloaders, criterion, optimizer_ft, scheduler, \n",
    "                        dataset_phases, dataset_sizes, device=device, num_epochs=25)\n",
    "visualize_model(fcn_model, dataloaders, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PyTorchSIIM]",
   "language": "python",
   "name": "conda-env-PyTorchSIIM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
