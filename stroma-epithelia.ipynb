{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread, imwrite\n",
    "\n",
    "import os, copy, time\n",
    "\n",
    "from horsetools import list_files\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "    \n",
    "class SegmentationDataset(Dataset):\n",
    "    IMG_EXTS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif')\n",
    "    \n",
    "    def _get_files(self, folder):\n",
    "        if os.path.isdir(folder):\n",
    "            return sorted(list_files(folder, valid_exts=self.IMG_EXTS))\n",
    "        else:\n",
    "            raise(RuntimeError('No folder named \"{}\" found.'.format(folder)))\n",
    "    \n",
    "    def __init__(self, root, labels, image_transforms=None, mask_transforms=None):\n",
    "        self.imgs = self._get_files(os.path.join(root, 'images'))\n",
    "        self.masks = self._get_files(os.path.join(root, 'masks'))\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.image_transforms = image_transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = default_loader(self.imgs[index])\n",
    "        mask = default_loader(self.masks[index])\n",
    "        if self.image_transforms is not None:\n",
    "            img = self.image_transforms(img)\n",
    "        if self.mask_transforms is not None:\n",
    "            mask = self.mask_transforms(mask)\n",
    "            \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find means and stds of dataset\n",
    "imgs_list = list_files('stroma_epithelia/images')\n",
    "np_means = np.zeros((len(imgs_list), 3))\n",
    "np_stds = np.zeros_like(np_means)\n",
    "for i, img_name in enumerate(imgs_list):\n",
    "    img = imread(img_name)\n",
    "    np_means[i] = np.mean(img, axis=(0, 1))\n",
    "    np_stds[i] = np.std(img, axis=(0, 1))\n",
    "    \n",
    "channel_means = np.mean(np_means, axis=0)\n",
    "channel_stds = np.std(np_stds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Means: {}'.format(channel_means))\n",
    "print('Stds: {}'.format(channel_stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'stroma_epithelia'\n",
    "crop_size = 224\n",
    "labels = (0, 1, 2)\n",
    "dataset_phases = ['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class LabelToOnehot(object):\n",
    "    def __init__ (self, labels):\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        \n",
    "        if len(img.shape) > 2:\n",
    "            img = img[:, :, 0]\n",
    "            \n",
    "        onehot = np.zeros((img.shape[0], img.shape[1], len(self.labels)))\n",
    "        for i, l in enumerate(self.labels):\n",
    "            onehot[:, :, i] = img == l\n",
    "            \n",
    "        return onehot\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': {\n",
    "        'imgs': transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(channel_means, channel_stds)\n",
    "        ]),\n",
    "        'masks': transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            LabelToOnehot(labels),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    }\n",
    "}\n",
    "\n",
    "image_datasets = {x: SegmentationDataset(data_root, labels, \n",
    "                                         image_transforms=data_transforms[x]['imgs'],\n",
    "                                         mask_transforms=data_transforms[x]['masks'])\n",
    "                  for x in dataset_phases}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4)\n",
    "               for x in dataset_phases}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in dataset_phases}\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test onehot \n",
    "mask_name = 'stroma_epithelia/masks/0.1_110_2_1.png'\n",
    "mask = imread(mask_name)\n",
    "LO = LabelToOnehot(labels)\n",
    "mask_oh = LO(mask)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(mask)\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, means=None, stds=None, title=None):\n",
    "    # convert tensor back to image range [0, 1]\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    if means is not None and stds is not None:\n",
    "        inp = np.array(stds) * inp + np.array(means)\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "print('Image shape: {}'.format(inputs.shape))\n",
    "print('Masks shape: {}'.format(masks.shape))\n",
    "imshow(torchvision.utils.make_grid(inputs), means=channel_means, stds=channel_stds)\n",
    "imshow(torchvision.utils.make_grid(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
